{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "832HvNP5jNvR",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6a86ee6a-b3f1-4929-c62c-cb81f726c848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-12 14:50:50--  http://www.ivan-sipiran.com/downloads/yolo.zip\n",
      "Resolving www.ivan-sipiran.com (www.ivan-sipiran.com)... 66.96.149.31\n",
      "Connecting to www.ivan-sipiran.com (www.ivan-sipiran.com)|66.96.149.31|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 233686170 (223M) [application/zip]\n",
      "Saving to: ‘yolo.zip’\n",
      "\n",
      "yolo.zip            100%[===================>] 222.86M  9.12MB/s    in 64s     \n",
      "\n",
      "2022-10-12 14:51:55 (3.47 MB/s) - ‘yolo.zip’ saved [233686170/233686170]\n",
      "\n",
      "Archive:  yolo.zip\n",
      "   creating: config/\n",
      "  inflating: config/coco.data        \n",
      "  inflating: config/coco.names       \n",
      "  inflating: config/download_weights.sh  \n",
      "  inflating: config/yolov3-tiny.cfg  \n",
      "  inflating: config/yolov3.cfg       \n",
      "  inflating: config/yolov3.weights   \n",
      "   creating: images/\n",
      "  inflating: images/blueangels-det.jpg  \n",
      "  inflating: images/blueangels.jpg   \n",
      "  inflating: images/Intersection-Counts.jpg  \n",
      "  inflating: images/olympic-trials.jpg  \n",
      "   creating: utils/\n",
      "  inflating: utils/datasets.py       \n",
      "  inflating: utils/parse_config.py   \n",
      "  inflating: utils/utils.py          \n",
      "   creating: utils/__pycache__/\n",
      "  inflating: utils/__pycache__/datasets.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/parse_config.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/utils.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/__init__.cpython-36.pyc  \n",
      "  inflating: models.py               \n",
      "  inflating: object_tracker.py       \n",
      "  inflating: sort.py                 \n"
     ]
    }
   ],
   "source": [
    "!wget www.ivan-sipiran.com/downloads/yolo.zip\n",
    "!unzip yolo.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mgJLSE--kAd7"
   },
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbBCWfvwkAeM",
    "outputId": "54aca571-574e-4917-ffb4-7fc93332a38f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "config_path='config/yolov3.cfg'\n",
    "weights_path='config/yolov3.weights'\n",
    "class_path='config/coco.names'\n",
    "img_size=416\n",
    "nms_thres=0.4\n",
    "\n",
    "# Load model and weights\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "classes = utils.load_classes(class_path)\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8XdoHMjP3qO",
    "outputId": "fc556140-3931-4ccc-f228-63d06a6d21f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorbike',\n",
       " 'aeroplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'sofa',\n",
       " 'pottedplant',\n",
       " 'bed',\n",
       " 'diningtable',\n",
       " 'toilet',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZD5gCfQykAej"
   },
   "outputs": [],
   "source": [
    "#Funcion para detectar objetos en una imagen\n",
    "def detect_image(img):\n",
    "    # scale and pad image\n",
    "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
    "    imw = round(img.size[0] * ratio)\n",
    "    imh = round(img.size[1] * ratio)\n",
    "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                        (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input_img = Variable(image_tensor.type(Tensor))\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    return detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O people.jpg\n",
    "!wget https://raw.githubusercontent.com/valbarriere/CC6204-Deep-Learning/master/Additional_Material/utils/traffic.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image and get detections\n",
    "\n",
    "* Change the type of image\n",
    "* Change the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "VHGj0TZxkAeu",
    "outputId": "1ea9190c-1842-44a8-f231-84f57a968e29"
   },
   "outputs": [],
   "source": [
    "conf_thres=0.05\n",
    "# img_path = \"people.jpg\"\n",
    "img_path = \"traffic.jpg\"\n",
    "\n",
    "prev_time = time.time()\n",
    "img = Image.open(img_path)\n",
    "detections = detect_image(img)\n",
    "inference_time = datetime.timedelta(seconds=time.time() - prev_time)\n",
    "print ('Inference Time: %s' % (inference_time))\n",
    "\n",
    "# Get bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "img = np.array(img)\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(1, figsize=(12,9))\n",
    "ax.imshow(img)\n",
    "\n",
    "pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "unpad_h = img_size - pad_y\n",
    "unpad_w = img_size - pad_x\n",
    "\n",
    "if detections is not None:\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    # browse detections and draw bounding boxes(\n",
    "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections.cpu():\n",
    "        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(bbox)\n",
    "        plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
    "                bbox={'color': color, 'pad': 0})\n",
    "plt.axis('off')\n",
    "# save image\n",
    "plt.savefig(img_path.replace(\".jpg\", \"-det.jpg\"), bbox_inches='tight', pad_inches=0.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with a more modern model to check the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlZF-I222Eiz"
   },
   "outputs": [],
   "source": [
    "im_name = \"traffic.jpg\"\n",
    "\n",
    "model = YOLO(\"yolo11m-seg.pt\")  # load an official segmentation model\n",
    "from PIL import Image\n",
    "im1 = Image.open(im_name)\n",
    "results = model.predict(source=im1, save=True) \n",
    "im2 = Image.open(results[0].save_dir+f'/{im_name}')\n",
    "im2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other models: pose detection, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
