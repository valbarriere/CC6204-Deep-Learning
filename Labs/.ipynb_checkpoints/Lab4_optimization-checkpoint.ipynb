{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KLR7CJwMk9Yq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0Elk5e4lJqL"
   },
   "outputs": [],
   "source": [
    "#Generation of oscillating sequence \n",
    "n = 50\n",
    "x = np.arange(n) *np.pi\n",
    "y = np.cos(x) * np.exp(x/100) - 10*np.exp(-0.01*x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a7H_amYpldZN"
   },
   "outputs": [],
   "source": [
    "#Function to compute the exponentially weighted average of a sequence\n",
    "def ewa(y,beta):\n",
    "    n = len(y)\n",
    "    zs = np.zeros(n)\n",
    "    z = 0\n",
    "    \n",
    "    #Use the formula for each element in the sequence\n",
    "    for i in range(n):\n",
    "        z = beta*z + (1-beta)*y[i]\n",
    "        zs[i] = z\n",
    "    \n",
    "    return zs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1suZVkHSmBAd"
   },
   "outputs": [],
   "source": [
    "#Exponentially weighted average with bias correction\n",
    "def ewabc(y, beta):\n",
    "    n = len(y)\n",
    "    zs = np.zeros(n)\n",
    "    z = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        z = beta*z + (1-beta)*y[i]\n",
    "        zc = z/(1 - beta**(i+1))\n",
    "        zs[i] = zc\n",
    "    return zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqV-tHBPmhoT"
   },
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.plot(x, ewa(y, beta), c='red', label='Exp. Weighted Avg.')\n",
    "plt.plot(x, ewabc(y, beta), c='orange', label='EWA with bias correction')\n",
    "plt.legend()\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unas explicaciones de un LLM! [Do not hestitate to use them :)](https://chatgpt.com/share/9d6df8b5-af11-4602-9911-d54810a91410)\n",
    "\n",
    "\"Exponentially Weighted Average with Bias Correction\" is a technique commonly used in optimization algorithms like Adam (Adaptive Moment Estimation) in machine learning. Let's break it down:\n",
    "\n",
    "### Exponentially Weighted Average (EWA)\n",
    "\n",
    "An exponentially weighted average is a way to calculate a moving average that gives more weight to recent data points and less weight to older ones. It is defined mathematically as:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) x_t\n",
    "$$\n",
    "\n",
    "* $v_t$​: The exponentially weighted average at time tt.\n",
    "* $\\beta$: The decay rate, a value between 0 and 1. A higher ββ gives more weight to older values.\n",
    "* $x_t$​: The current data point.\n",
    "\n",
    "### Bias Correction\n",
    "\n",
    "In the early stages of computing the exponentially weighted average, especially for small datasets, the average can be biased towards zero because the initial values are underestimated. To correct this bias, the following adjustment is made:\n",
    "$$\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta^t}\n",
    "$$ \n",
    "\n",
    "* $\\hat{v}_t$: The bias-corrected exponentially weighted average.\n",
    "* $\\beta^t$: The term that corrects for the bias introduced in the early stages.\n",
    "\n",
    "### Why is Bias Correction Needed?\n",
    "\n",
    "When you start averaging, the initial values of the exponentially weighted average tend to be smaller than they should be because they don't have enough history. Bias correction adjusts for this by normalizing the average based on the number of terms you've averaged so far.\n",
    "\n",
    "### Application in Optimization Algorithms\n",
    "\n",
    "In optimization algorithms like Adam, exponentially weighted averages are used to smooth out the gradients during training. Bias correction ensures that these averages are accurate even in the initial steps of the training process, leading to more reliable updates to the model's parameters.\n",
    "\n",
    "This technique helps in achieving faster convergence and more stable training in neural networks and other machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y8yPpLfcm3IS"
   },
   "outputs": [],
   "source": [
    "#We define the x^2 function to examplify\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def grad(x):\n",
    "    return 2*x\n",
    "\n",
    "#Classic gradient descent\n",
    "def gd(x, grad, alpha, max_iter=10):\n",
    "    xs = np.zeros(1 + max_iter)\n",
    "    xs[0] = x\n",
    "    for i in range(max_iter):\n",
    "        x = x - alpha*grad(x)\n",
    "        xs[i+1] = x\n",
    "\n",
    "    return xs\n",
    "\n",
    "#Gradient descent with momentum\n",
    "def gd_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n",
    "    xs = np.zeros(1 + max_iter)\n",
    "    xs[0] = x\n",
    "\n",
    "    v=0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)\n",
    "        vc = v/(1 + beta**(i+1))\n",
    "        x = x - alpha*vc\n",
    "        xs[i+1] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfvM-oc5ok6a"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with moderate step size\n",
    "alpha = 0.1\n",
    "x0 = 1\n",
    "xs = gd(x0, grad, alpha)\n",
    "\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.figure()\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "\n",
    "for i, (x,y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x, y+0.2, i, bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-dRecqIpOHs"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with large step size\n",
    "alpha = 0.95\n",
    "x0 = 1\n",
    "xs = gd(x0, grad, alpha)\n",
    "\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.figure()\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "\n",
    "for i, (x,y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x*1.2, y, i, bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nolExzt0pnf3"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with momentum\n",
    "alpha = 0.95\n",
    "x0 = 1\n",
    "xs = gd_momentum(x0, grad, alpha, beta=0.9, max_iter=5)\n",
    "\n",
    "xp = np.linspace(-1.2, 1.2, 100)\n",
    "plt.figure()\n",
    "plt.plot(xp, f(xp))\n",
    "plt.plot(xs, f(xs), 'o-', c='red')\n",
    "\n",
    "for i, (x,y) in enumerate(zip(xs, f(xs)), 1):\n",
    "    plt.text(x, y+0.2, i, bbox=dict(facecolor='yellow', alpha=0.5), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktDdJfoHqhkQ"
   },
   "source": [
    "# Momentum + RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qGg5zQFqqLXu"
   },
   "outputs": [],
   "source": [
    "#Definition of function in 2D\n",
    "\n",
    "def f2(x):\n",
    "    return x[0]**2 + 100*x[1]**2\n",
    "\n",
    "def grad2(x):\n",
    "    return np.array([2*x[0], 200*x[1]])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9cp7IlHqzYH"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X,Y = np.meshgrid(x, y)\n",
    "levels=[0.1, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = X**2 + 100*Y**2\n",
    "plt.figure()\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Tift4Sk5rUmt"
   },
   "outputs": [],
   "source": [
    "#Classic gradient descent\n",
    "def gd2(x, grad, alpha, max_iter=10):\n",
    "    xs = np.zeros((1+max_iter,x.shape[0]))\n",
    "    xs[0,:] = x\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x = x - alpha*grad(x)\n",
    "        xs[i+1,:] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BY5pR8M3sBHp"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with momentum\n",
    "def gd2_momentum(x, grad, alpha, beta=0.9, max_iter=10):\n",
    "    xs = np.zeros((1+max_iter, x.shape[0]))\n",
    "    xs[0,:] = x\n",
    "\n",
    "    v = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)\n",
    "        vc = v/(1 + beta**(i+1))\n",
    "        x = x - alpha*vc\n",
    "        xs[i+1,:] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB_8DJ1zsa87"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with large step size\n",
    "alpha = 0.01\n",
    "x0 = np.array([-1, 1])\n",
    "xs = gd2(x0, grad2, alpha, max_iter=75)\n",
    "\n",
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels=[0.1, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "plt.figure()\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:,0],xs[:,1], 'o-', c='red')\n",
    "plt.title('Vanilla gradient descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_Wb6Pc_tIAe"
   },
   "outputs": [],
   "source": [
    "#Gradient descent with momentum\n",
    "alpha = 0.01\n",
    "x0 = np.array([-1, 1])\n",
    "xs = gd2_momentum(x0, grad2, alpha, beta=0.9, max_iter=75)\n",
    "\n",
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels=[0.1, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "plt.figure()\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:,0],xs[:,1], 'o-', c='red')\n",
    "plt.title('Gradient descent with Momentum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Vn4fDbc0thmq"
   },
   "outputs": [],
   "source": [
    "#RMSProp\n",
    "def gd2_rmsprop(x, grad, alpha, beta=0.9, eps=1e-8, max_iter=10):\n",
    "    xs = np.zeros((1+max_iter, x.shape[0]))\n",
    "    xs[0,:] = x\n",
    "    v = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        v = beta*v + (1-beta)*grad(x)**2\n",
    "        x = x - alpha*grad(x)/(eps + np.sqrt(v))\n",
    "        xs[i+1,:] = x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ERZDc5jurbT"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x0 = np.array([-1, 1])\n",
    "xs = gd2_rmsprop(x0, grad2, alpha, beta=0.9, max_iter=10)\n",
    "\n",
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels=[0.1, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "plt.figure()\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:,0],xs[:,1], 'o-', c='red')\n",
    "plt.title('RMSProp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHLoC4ETwZWu"
   },
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gJHRJQ0ru2Nw"
   },
   "outputs": [],
   "source": [
    "def gd2_adam(x, grad, alpha, beta1=0.9, beta2=0.999, eps=1e-8, max_iter=10):\n",
    "    xs = np.zeros((1+max_iter,x.shape[0]))\n",
    "    xs[0,:] = x\n",
    "    m=0\n",
    "    v=0\n",
    "    for i in range(max_iter):\n",
    "        m = beta1*m + (1-beta1)*grad(x)\n",
    "        v = beta2*v + (1-beta2)*grad(x)**2\n",
    "        mc = m/(1 + beta1**(i+1))\n",
    "        vc = v/(1 + beta2**(i+1))\n",
    "        x = x - alpha*m/(eps + np.sqrt(vc))\n",
    "        xs[i+1,:]=x\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG4bVa0Rxh0X"
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x0 = np.array([-1, 1])\n",
    "xs = gd2_adam(x0, grad2, alpha, beta1=0.9, beta2=0.99, max_iter=5)\n",
    "\n",
    "x = np.linspace(-1.2, 1.2, 100)\n",
    "y = np.linspace(-1.2, 1.2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels=[0.1, 1, 2, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n",
    "Z = x**2 + 100*Y**2\n",
    "plt.figure()\n",
    "c = plt.contour(X, Y, Z, levels)\n",
    "plt.plot(xs[:,0],xs[:,1], 'o-', c='red')\n",
    "plt.title('Adam')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kH_ueeP9MRK3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4_optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
