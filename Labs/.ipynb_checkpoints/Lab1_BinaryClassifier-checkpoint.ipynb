{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fETNEqpGU8Q8"
      },
      "source": [
        "# Binary classifier with a sigmoid neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNn6p99NU8Q_"
      },
      "source": [
        "Let's implement a simple sigmoid neuron to classify 2D points. First, we only need two libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN5Sai0TU8Q_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYphxd9U8RA"
      },
      "source": [
        "We will create two random distributions of points. One of the distribution is shifted, so there is a clear linear separation between both sets. We build a dataset with the points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKijPSPVU8RA"
      },
      "outputs": [],
      "source": [
        "#Let's create a dataset of 2D points. The distribution is only shifted 3.5 units\n",
        "X1 = np.random.randn(1000, 2)\n",
        "X2 = np.random.randn(1000, 2) + 3.5\n",
        "\n",
        "#The first 750 points of each distribution go in the training set\n",
        "X_train = np.concatenate([X1[:750,:], X2[:750,:]], axis=0)\n",
        "\n",
        "#The last 250 points of each distribution go in the test set\n",
        "X_test = np.concatenate([X1[750:,:], X2[750:,:]], axis=0)\n",
        "\n",
        "#We also create the labels\n",
        "Y1 = np.zeros((1000, 1)) #The points in the first distribution have class 0\n",
        "Y2 = np.ones((1000, 1)) # The points in the second distribution have class 1\n",
        "\n",
        "#The first 750 labels of each distribution go in the training set\n",
        "Y_train = np.concatenate([Y1[:750], Y2[:750]], axis=0)\n",
        "\n",
        "#The last 250 labels of each distribution go in the test set\n",
        "Y_test = np.concatenate([Y1[750:], Y2[750:]], axis=0)\n",
        "\n",
        "#Plot the training set\n",
        "plt.scatter(X_train[:750,0], X_train[:750,1], color='b')\n",
        "plt.scatter(X_train[750:,0], X_train[750:,1], color='r')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX4t6rE-U8RB"
      },
      "source": [
        "We develop the neural network and the gradient descent algorithm for learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v4JvB5BU8RC"
      },
      "outputs": [],
      "source": [
        "#Definition of sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "#Let's implement the gradient descent algorithm\n",
        "\n",
        "#Initialize random parameters\n",
        "w1 = 2*np.random.rand(1) - 1\n",
        "w2 = 2*np.random.rand(1) - 1\n",
        "b = 2*np.random.rand(1) - 1\n",
        "\n",
        "#Initialize hiper-parameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "#Log lists\n",
        "log_train = []\n",
        "log_test = []\n",
        "\n",
        "#Iterate over the epochs\n",
        "for i in range(num_epochs):\n",
        "    cost = 0\n",
        "    \n",
        "    #Foe each epoch, iterate over the training samples\n",
        "    for j in range(X_train.shape[0]):\n",
        "        #Compute the linear function\n",
        "        z = w1 * X_train[j,0] + w2 * X_train[j, 1] + b\n",
        "        #Compute the activation function (sigmoid)\n",
        "        y1 = sigmoid(z)\n",
        "        \n",
        "        #Compute error and cost for this sample\n",
        "        error = y1 - Y_train[j]\n",
        "        cost = cost + error**2\n",
        "        \n",
        "        #Compute the derivatives for each parameter        \n",
        "        dw1 = error*y1*(1-y1)*X_train[j,0]\n",
        "        dw2 = error*y1*(1-y1)*X_train[j,1]\n",
        "        db = error*y1*(1-y1)\n",
        "        \n",
        "        #Perform the update rules        \n",
        "        w1 = w1 - learning_rate*dw1\n",
        "        w2 = w2 - learning_rate*dw2\n",
        "        b = b - learning_rate*db\n",
        "    \n",
        "    #At the end of the epoch, we compute the total cost of the epoch\n",
        "    cost = cost/(2*X_train.shape[0])\n",
        "    log_train.append(cost)\n",
        "    \n",
        "    #We verify how good is the model so far, by computing the accuracy in the test set\n",
        "    test_output = []\n",
        "    cost = 0\n",
        "    \n",
        "    #Iterate over the test samples\n",
        "    for j in range(X_test.shape[0]):\n",
        "        #Compute the output of the neuron\n",
        "        y1 = sigmoid(w1 * X_test[j,0] + w2 * X_test[j, 1] + b)\n",
        "        cost = cost + (y1 - Y_test[j])**2\n",
        "        test_output.append(y1)\n",
        "    \n",
        "    #Average the cost over the test set\n",
        "    cost = cost / (2*X_test.shape[0])\n",
        "    log_test.append(cost)\n",
        "    \n",
        "    test_output = np.asarray(test_output)\n",
        "    test_output = (test_output >= 0.5)\n",
        "    \n",
        "    #Compare the output of the network with the correct labels\n",
        "    correct = (test_output == Y_test).sum()\n",
        "    \n",
        "    print('Epoch {}/{}: loss = {}: Test accuracy={}'.format(i, num_epochs, cost, (100*correct)/test_output.shape[0]))\n",
        "    \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MZiYhArU8RD"
      },
      "outputs": [],
      "source": [
        "#Plot of historic logs\n",
        "plt.plot(log_train, color='b')\n",
        "plt.plot(log_test, color='r')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdi34ZgZU8RD"
      },
      "outputs": [],
      "source": [
        "#Plot the boundary decision\n",
        "\n",
        "x1_val = np.linspace(X_train[:,0].min(), X_train[:,0].max(), 100)\n",
        "x2_val = (-b - (w1 * x1_val))/w2\n",
        "\n",
        "\n",
        "plt.scatter(X_train[:750, 0], X_train[:750, 1], color='b')\n",
        "plt.scatter(X_train[750:, 0], X_train[750:, 1], color='r')\n",
        "plt.plot(x1_val, x2_val, 'g')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "BinaryClassifier .ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}